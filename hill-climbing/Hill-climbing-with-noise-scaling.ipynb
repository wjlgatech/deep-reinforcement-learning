{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hill Climbing with Adaptive Noise Scaling\n",
    "\n",
    "## 1. Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvirtualdisplay in /Users/jialiangwu/anaconda3/lib/python3.7/site-packages (1.3.2)\r\n",
      "Requirement already satisfied: EasyProcess in /Users/jialiangwu/anaconda3/lib/python3.7/site-packages (from pyvirtualdisplay) (0.3)\r\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyvirtualdisplay'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7247b18eb788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python -m pip install pyvirtualdisplay'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyvirtualdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m900\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyvirtualdisplay'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "!python -m pip install pyvirtualdisplay\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Instantiate Env and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(2,)\n",
      "action space: Box(1,)\n",
      "  - low: [-1.]\n",
      "  - high: [1.]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.seed(101)\n",
    "np.random.seed(101)\n",
    "\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "print('  - low:', env.action_space.low)\n",
    "print('  - high:', env.action_space.high)\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, env, h_size=16):\n",
    "        super(Agent, self).__init__()\n",
    "        self.env = env\n",
    "        # state, hidden layer, action sizes\n",
    "        self.s_size = env.observation_space.shape[0]\n",
    "        self.h_size = h_size\n",
    "        self.a_size = env.action_space.shape[0]\n",
    "        # define layers\n",
    "        self.fc1 = nn.Linear(self.s_size, self.h_size)\n",
    "        self.fc2 = nn.Linear(self.h_size, self.a_size)\n",
    "        \n",
    "    def set_weights(self, weights):\n",
    "        s_size = self.s_size\n",
    "        h_size = self.h_size\n",
    "        a_size = self.a_size\n",
    "        # separate the weights for each layer\n",
    "        fc1_end = (s_size*h_size)+h_size\n",
    "        fc1_W = torch.from_numpy(weights[:s_size*h_size].reshape(s_size, h_size))\n",
    "        fc1_b = torch.from_numpy(weights[s_size*h_size:fc1_end])\n",
    "        fc2_W = torch.from_numpy(weights[fc1_end:fc1_end+(h_size*a_size)].reshape(h_size, a_size))\n",
    "        fc2_b = torch.from_numpy(weights[fc1_end+(h_size*a_size):])\n",
    "        # set the weights for each layer\n",
    "        self.fc1.weight.data.copy_(fc1_W.view_as(self.fc1.weight.data))\n",
    "        self.fc1.bias.data.copy_(fc1_b.view_as(self.fc1.bias.data))\n",
    "        self.fc2.weight.data.copy_(fc2_W.view_as(self.fc2.weight.data))\n",
    "        self.fc2.bias.data.copy_(fc2_b.view_as(self.fc2.bias.data))\n",
    "    \n",
    "    def get_weights_dim(self):\n",
    "        return (self.s_size+1)*self.h_size + (self.h_size+1)*self.a_size\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        return x.cpu().data #action\n",
    "        \n",
    "    def evaluate(self, weights, gamma=1.0, max_t=5000):\n",
    "        self.set_weights(weights) #update agent nn weights\n",
    "        episode_return = 0.0\n",
    "        state = self.env.reset()\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "            action = self.forward(state)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            episode_return += reward * math.pow(gamma, t)\n",
    "            if done:\n",
    "                break\n",
    "        return episode_return\n",
    "    \n",
    "agent = Agent(env).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.48382922,  0.        ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Agent with Hill Climbing with Adaptive Noise Scaling\n",
    "\n",
    "As explained [here](https://towardsdatascience.com/three-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b), \n",
    "\n",
    ">The adaptive noise scaling for our model is realized as follows. If the current value of the target function is better than the best value obtained for the target function, we divide the noise scale by 2, and this noise is added to the weight matrix. If the current value of the target function is worse than the best obtained value, we multiply the noise scale by 2, and this noise is added to the best obtained value of the weight matrix. In both cases, a noise scale is added with some random factor different for any element of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: -1.49\n",
      "performance improving=>decreasing sigma 0.25\n",
      "Episode 20\tAverage Score: -1.08\n",
      "performance improving=>decreasing sigma 0.125\n",
      "Episode 30\tAverage Score: -0.73\n",
      "performance worsen=>increasing sigma 0.25\n",
      "Episode 40\tAverage Score: -0.60\n",
      "performance worsen=>increasing sigma 0.5\n",
      "Episode 50\tAverage Score: -0.78\n",
      "performance worsen=>increasing sigma 1.0\n",
      "Episode 60\tAverage Score: -1.72\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 70\tAverage Score: -8.17\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 80\tAverage Score: -8.31\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 90\tAverage Score: -8.22\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 100\tAverage Score: -8.03\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 110\tAverage Score: -8.23\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 120\tAverage Score: -9.00\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 130\tAverage Score: -9.53\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 140\tAverage Score: -9.93\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 150\tAverage Score: -10.30\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 160\tAverage Score: -10.20\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 170\tAverage Score: -6.17\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 180\tAverage Score: -5.79\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 190\tAverage Score: -5.54\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 200\tAverage Score: -5.30\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 210\tAverage Score: -5.33\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 220\tAverage Score: -4.86\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 230\tAverage Score: -4.77\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 240\tAverage Score: -4.76\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 250\tAverage Score: -4.63\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 260\tAverage Score: -4.47\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 270\tAverage Score: -4.30\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 280\tAverage Score: -4.39\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 290\tAverage Score: -4.50\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 300\tAverage Score: -4.68\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 310\tAverage Score: -4.71\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 320\tAverage Score: -4.67\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 330\tAverage Score: -4.64\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 340\tAverage Score: -4.79\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 350\tAverage Score: -4.85\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 360\tAverage Score: -4.88\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 370\tAverage Score: -4.68\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 380\tAverage Score: -4.48\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 390\tAverage Score: -4.09\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 400\tAverage Score: -3.87\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 410\tAverage Score: -3.82\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 420\tAverage Score: -3.96\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 430\tAverage Score: -3.93\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 440\tAverage Score: -3.66\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 450\tAverage Score: -3.54\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 460\tAverage Score: -3.71\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 470\tAverage Score: -4.01\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 480\tAverage Score: -3.98\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 490\tAverage Score: -4.21\n",
      "performance worsen=>increasing sigma 2\n",
      "Episode 500\tAverage Score: -4.35\n",
      "performance worsen=>increasing sigma 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "policy = Policy()\n",
    "\"\"\"\n",
    "def hill_climbing(agent, n_episodes=1000, max_t=1000, gamma=1.0, print_every=10, noise_scale=0.5, sigma=0.5):\n",
    "    \"\"\"Implementation of hill climbing with adaptive noise scaling.\n",
    "        \n",
    "    Params\n",
    "    ======\n",
    "        - n_episodes (int): maximum number of training episodes\n",
    "        - max_t (int): maximum number of timesteps per episode\n",
    "        - gamma (float): discount rate\n",
    "        - print_every (int): how often to print average score (over last 100 episodes)\n",
    "        - noise_scale (float): standard deviation of additive noise\n",
    "        - sigma (float): standard deviation of additive noise\n",
    "    \"\"\"\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    best_R = -np.Inf\n",
    "    #replace: best_w = policy.w\n",
    "    best_w = noise_scale*np.random.randn(agent.get_weights_dim())\n",
    "    agent.set_weights(best_w)\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        rewards = []\n",
    "        state = agent.env.reset()\n",
    "        rewards = np.array([agent.evaluate(best_w, gamma, max_t) for weights in weights_pop])\n",
    "        \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        discounts = [gamma**i for i in range(len(rewards)+1)]\n",
    "        R = sum([discount*reward for discount,reward in zip(discounts, rewards)])\n",
    "        print('noise_scale=', noise_scale)\n",
    "        if R >= best_R: # found better weights\n",
    "            best_R = R\n",
    "            noise_scale = max(1e-3, noise_scale / 2)\n",
    "            best_w += noise_scale * np.random.rand(agent.get_weights_dim()) \n",
    "            agent.set_weights(best_w)\n",
    "    \n",
    "        else: # did not find better weights\n",
    "            noise_scale = min(2, noise_scale * 2)\n",
    "            best_w += noise_scale * np.random.rand(agent.get_weights_dim())\n",
    "            agent.set_weights(best_w)\n",
    "    \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            agent.set_weights(best_w)\n",
    "            break\n",
    "        \n",
    "    return scores\n",
    "\n",
    "def cem_adaptive_scaling_noise(n_iterations=500, max_t=1000, gamma=1.0, print_every=10, pop_size=50, elite_frac=0.2, sigma=0.5, is_adaptive=False):\n",
    "    \"\"\"PyTorch implementation of the cross-entropy method.\n",
    "        \n",
    "    Params\n",
    "    ======\n",
    "        n_iterations (int): maximum number of training iterations\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        gamma (float): discount rate\n",
    "        print_every (int): how often to print average score (over last 100 episodes)\n",
    "        pop_size (int): size of population at each iteration\n",
    "        elite_frac (float): percentage of top performers to use in update\n",
    "        sigma (float): standard deviation of additive noise\n",
    "    \"\"\"\n",
    "    n_elite=int(pop_size*elite_frac)\n",
    "\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    best_weight = sigma*np.random.randn(agent.get_weights_dim())\n",
    "    best_reward = -np.Inf\n",
    "    \n",
    "    for i_iteration in range(1, n_iterations+1):\n",
    "        weights_pop = [best_weight + (sigma*np.random.randn(agent.get_weights_dim())) for i in range(pop_size)]\n",
    "        rewards = np.array([agent.evaluate(weights, gamma, max_t) for weights in weights_pop])\n",
    "\n",
    "        elite_idxs = rewards.argsort()[-n_elite:]\n",
    "        elite_weights = [weights_pop[i] for i in elite_idxs]\n",
    "        best_weight = np.array(elite_weights).mean(axis=0)\n",
    "\n",
    "        reward = agent.evaluate(best_weight, gamma=1.0)\n",
    "        scores_deque.append(reward)\n",
    "        scores.append(reward)\n",
    "        \n",
    "\n",
    "        \n",
    "        torch.save(agent.state_dict(), 'checkpoint.pth')\n",
    "        \n",
    "        if i_iteration % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_iteration, np.mean(scores_deque)))\n",
    "            if is_adaptive:\n",
    "                # adaptive scaling noise\n",
    "                if np.mean(scores_deque) >= best_reward:\n",
    "                    best_reward = reward\n",
    "                    sigma = max(1e-3, sigma / 2)\n",
    "                    print('performance improving=>decreasing sigma',sigma)\n",
    "                else:\n",
    "                    sigma = min(2, sigma*2)\n",
    "                    print('performance worsen=>increasing sigma',sigma)\n",
    "\n",
    "        if np.mean(scores_deque)>=90.0:\n",
    "            print('\\nEnvironment solved in {:d} iterations!\\tAverage Score: {:.2f}'.format(i_iteration-100, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = cem_adaptive_scaling_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
