{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation Project Report\n",
    "\n",
    "In this project, I train a DQN agent to navigate in a large, square world and to collect bananas.  \n",
    "\n",
    "\n",
    "## Problem Overview\n",
    "\n",
    "**State Space**: The state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  \n",
    "\n",
    "**Action Space**: Four discrete actions are available, corresponding to:\n",
    "- **`0`** - move forward.\n",
    "- **`1`** - move backward.\n",
    "- **`2`** - turn left.\n",
    "- **`3`** - turn right.\n",
    "\n",
    "**Reward**: A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana.  Thus, the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas.  \n",
    "\n",
    "**Problem Solved**: The task is episodic. The environment is considered solved when the agent get an average score of +13 over 100 consecutive episodes.\n",
    "\n",
    "\n",
    "## Method\n",
    "\n",
    "DQN uses a deep neural network to approximate the Q-value function. For each state, it estimates the q-value of each action and performs a gradient decent on the MSE loss between the expected Q-value and the current Q-value.  To stabilize and improve the DQN training procedure, two techniques are employed:\n",
    "\n",
    "**Experience Replay**: When we feed the experience tuples $(s,a,r,s')$ sequentially to train the neural network, there exists a correlationship between two consecutive tuples. To avoid this, we store experience tuples in a replay buffer and randomly sample a batch to calculate the expected value function.\n",
    "\n",
    "**Fixed Q-targets**: We use two networks: local and target. The local network is updated at every gradient step while the target network is updated with the current weights of the local network at regular interval.\n",
    "\n",
    "### DQN architecture\n",
    "\n",
    "The DQN networks has 3 fully connected layers, each with 256 neurons. Each layer if followed by a ReLu activation function. To avoid overfitting, dropout with a probability of $50\\%$ is used after each fully connected layer except the last one.\n",
    "\n",
    "The network accepts a tensor of 37 dimensions which is the dimension of each state; the network returns a tensor of 4 dimension which is the number of action an agent can perform at each state.\n",
    "\n",
    "### DQN hyperparameters\n",
    "\n",
    "- Replay buffer size: $e^5$\n",
    "\n",
    "- Batch size: 64\n",
    "\n",
    "- Discount factor (gamma): 0.99\n",
    "\n",
    "- Soft update parameter (TAU): $e^{-3}$\n",
    "\n",
    "- Learning rate (alpha): $1e^{-3}$\n",
    "\n",
    "- Frequency of network update: 50\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "The DQN agent is able to solve the environment in 2866 episodes. After __ episodes, the score did not improve much until about __ episode, the score restarts increasing.\n",
    "![average_socres](average_socres.png)\n",
    "\n",
    "## Next Steps\n",
    "The performance of the current DQN model is quite sensitive to its hyperparameters. It would be extremely valuable to have a systematic way to do hyperparameter tuning and find ways to control/decrease the sensitivity of model hyperparameters. \n",
    "\n",
    "Moreover, the current DQN model samples experience uniformly from the replay buffer, assuming that each experience has the same priority. In reality, the experience with bigger TD-error indicates that the neural network can learn more than from them. Thus we can associate each experience with a priority score which is monotone increasing function of TD-error and sample them based on the score. This will help to select important experiences that may be rare and easily got ignored by the uniform sampling method.\n",
    "\n",
    "- Prioritized Experience Replay: https://arxiv.org/abs/1511.05952\n",
    "\n",
    "Other than Prioritized Experience Replay, there are other extensions of DQN: Double-DQN, Dueling-DQN. I will look into these research to see the edges they add to the vanilla DQN.\n",
    "\n",
    "- Dueling DQN: https://arxiv.org/abs/1511.06581\n",
    "\n",
    "- Double-Q Learning: https://arxiv.org/abs/1509.06461\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
